% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_prompting.R
\name{prompt_llm}
\alias{prompt_llm}
\title{Interrogate a Language Model}
\usage{
prompt_llm(
  messages = NULL,
  provider = getOption("llmr_llm_provider"),
  params = list(temperature = 0),
  force_json = FALSE,
  log_request = getOption("llmr_log_requests", TRUE),
  session_id = get_session_id() \%||\% set_session_id(),
  ...
)
}
\arguments{
\item{messages}{Messages to be sent to the language model. Can be a single
string, a named vector, or a list of messages. See \code{process_messages()} for
details.}

\item{provider}{The provider of the language model. Default is set by the
\code{llmr_llm_provider} option. Supported providers include "openai", "azure",
"gemini", and "custom".}

\item{params}{Additional parameters for the language model request. Defaults
to a list with \code{temperature = 0}.}

\item{force_json}{A boolean to force the response in JSON format. Default is
FALSE. Note: This is not supported by all providers.}

\item{log_request}{A boolean to log the request time. Default is set by the
\code{llmr_log_requests} option.}

\item{session_id}{The LLM session ID to store the data under. If not set, a
new one will be created.}

\item{...}{Additional arguments passed to the language model provider
functions.}
}
\value{
The content of the message from the language model response.
}
\description{
This function sends requests to a specified language model provider and
returns the response. It handles rate limiting, retries requests if
necessary, and processes errors in the response.
}
\section{Error Handling}{
 The function automatically handles rate limit errors
and will retry the request after waiting for the specified time. For
responses cut off due to token limits, the function will attempt to
complete the response, which may involve user interaction.
}

\section{JSON Output}{
 When \code{force_json = TRUE}, the function attempts to get
a JSON response from the LLM. If parsing fails, it tries to sanitize the
output.
}

\examples{
\dontrun{
response <- prompt_llm(messages = c(user = "Hello there!"), provider = "openai")
response <- prompt_llm(
  messages = list(list(role = "user", content = "What's the weather?")),
  params = list(temperature = 0.7)
)
}

}

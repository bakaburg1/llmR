% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_prompting.R
\name{prompt_llm}
\alias{prompt_llm}
\title{Interrogate a Language Model}
\usage{
prompt_llm(
  messages = NULL,
  provider = getOption("llmr_llm_provider"),
  params = list(),
  model_specification = NULL,
  force_json = FALSE,
  log_request = getOption("llmr_log_requests", TRUE),
  session_id = get_session_id() \%||\% set_session_id(),
  ...
)
}
\arguments{
\item{messages}{Messages to be sent to the language model. Can be a single
string, a named vector, or a list of messages. See \code{process_messages()} for
details.}

\item{provider}{The provider of the language model. Default is set by the
\code{llmr_llm_provider} option. Supported providers include "openai", "azure",
"gemini", and "custom".}

\item{params}{Additional parameters for the language model request. These
will override any default parameters stored with the model specification.}

\item{model_specification}{Either a model label (string) that has been
registered with \code{record_llmr_model()}, or a complete model specification
list containing the same fields as \code{record_llmr_model()}. If provided, this
overrides the current model set by \code{set_llmr_model()}.}

\item{force_json}{A boolean to force the response in JSON format. Default is
FALSE. Note: This is not supported by all providers.}

\item{log_request}{A boolean to log the request time. Default is set by the
\code{llmr_log_requests} option.}

\item{session_id}{The LLM session ID to store the data under. If not set, a
new one will be created.}

\item{...}{Additional arguments passed to the language model provider
functions.}
}
\description{
This function sends requests to a specified language model provider and
returns the response. It handles rate limiting, retries requests if
necessary, and processes errors in the response.
}
\section{Error Handling}{
 The function automatically handles rate limit errors
and will retry the request after waiting for the specified time. For
responses cut off due to token limits, the function will attempt to
complete the response, which may involve user interaction.
}

\section{JSON Output}{
 When \code{force_json = TRUE}, the function attempts to get
a JSON response from the LLM. If parsing fails, it tries to sanitize the
output.
}

\examples{
\dontrun{
# First record a model specification
record_llmr_model(
  label = "openai",
  provider = "openai",
  model = "gpt-4",
  api_key = "your_api_key",
  parameters = list(temperature = 0.7)
)

# Then use the registered model label
response <- prompt_llm(
  # Shorthand message format
  messages = c(user = "Hello there!"), # or simply "Hello there!"
  model_specification = "openai"
)

# Using a complete model specification
response <- prompt_llm(
  # Canonical message format
  messages = list(list(role = "user", content = "What's the weather?")),
  model_specification = list(
    provider = "openai",
    model = "gpt-4",
    api_key = "your_api_key",
    parameters = list(temperature = 0.7)
  )
)
}

}

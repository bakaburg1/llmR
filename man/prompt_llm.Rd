% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LLM_prompting.R
\name{prompt_llm}
\alias{prompt_llm}
\title{Interrogate a Language Model}
\usage{
prompt_llm(
  messages = NULL,
  provider = getOption("llmr_llm_provider"),
  params = list(temperature = 0),
  force_json = FALSE,
  log_request = getOption("llmr_log_requests", TRUE),
  session_id = get_session_id() \%||\% set_session_id(),
  ...
)
}
\arguments{
\item{messages}{Messages to be sent to the language model.}

\item{provider}{The provider of the language model. Maps to a specific
function with the pattern "use_\if{html}{\out{<provider>}}_llm. Default is set up globally
using the \code{llmr_llm_provider} option. The package implements interfaces for
models respecting the OpenAI (\code{openai}), Azure (\code{azure}), and Google Gemini
(\code{gemini}) API specifications plus a general interface for custom models
(\code{custom}) which implement the OpenAI API specification; see
\verb{use_<provider>_llm} functions.}

\item{params}{Additional parameters for the language model request. Defaults
to a list with \code{temperature = 0}.}

\item{force_json}{A boolean to force the response in JSON format. Default is
FALSE. Works only for OpenAI and Azure endpoints.}

\item{log_request}{A boolean to log the request time. Can be set up globally
using the \code{llmr_log_requests} option, which defaults to TRUE.}

\item{session_id}{The LLM session ID to store the data under. If not set
already globally, a new one will be created. NOTE: this ID is not used to
continue a conversation keeping memory of the previous interactions with
the LLM, but it's just to keep a copy of the conversation for review and
post-processing.}

\item{...}{Additional arguments passed to the language model provider
functions.}
}
\value{
Returns the content of the message from the language model response.
}
\description{
This function sends requests to a specified language model provider (OpenAI,
Azure, or a locally running LLM server) and returns the response. It handles
rate limiting and retries the request if necessary, and also processes errors
in the response.
}
\details{
Users can provide their own models by writing a function with the following
name pattern: \verb{use_<model_name>_llm}. See the existing functions using the
::: operator for examples.
}
\examples{
\dontrun{
response <- prompt_llm(
 messages = c(user = "Hello there!"),
 provider = "openai")
 }

}
